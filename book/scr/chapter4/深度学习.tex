\subsection{神经网络模型}
在深度学习十分火热的今天，不时会涌现出各种新型的人工神经网络，想要实时了解这些新型神经网络的架构还真是不容易。光是知道各式各样的神经网络模型缩写（如：DCIGN、BiLSTM、DCGAN……还有哪些？)，就已经让人招架不住了。

因此，这里整理出一份清单来梳理所有这些架构。其中大部分是人工神经网络，也有一些完全不同的怪物。尽管所有这些架构都各不相同、功能独特，当我在画它们的节点图时……其中潜在的关系开始逐渐清晰起来。

把这些架构做成节点图，会存在一个问题：它无法展示神经网络架构内部的工作原理。举例来说，变分自编码机（VAE：variational autoencoders ）看起来跟自编码机（AE：autoencoders）差不多，但它们的训练过程却大不相同。训练后的模型在使用场景上差别更大：VAE是生成器，通过插入噪音数据来获取新样本；而AE仅仅是把他们所收到的任何信息作为输入，映射到“记忆中”最相似的训练样本上。

在介绍不同模型的神经元和神经细胞层之间的连接方式前，我们一步一步来，先来了解不同的神经元节点内部是如何工作的。
\subsubsection{神经元}
对不同类型的神经元标记不同的颜色，可以更好地在各种网络架构之间进行区分。但是，这些神经元的工作方式却是大同小异。在下图的基本神经元结构后面，你会看到详细的讲解：

基本的人工神经网络神经元（basic neural network cell）相当简单，这种简单的类型可以在常规的前馈人工神经网络架构里面找到。这种神经元与其它神经元之间的连接具有权重，也就是说，它可以和前一层神经网络层中的所有神经元有连接。

每一个连接都有各自的权重，通常情况下是一些随机值（关于如何对人工神经网络的权重进行初始化是一个非常重要的话题，这将会直接影响到之后的训练过程，以及最终整个模型的性能）。这个权重可以是负值，正值，非常小，或者非常大，也可以是零。和这个神经元连接的所有神经元的值都会乘以各自对应的权重。然后，把这些值都求和。

在这个基础上，会额外加上一个bias，它可以用来避免输出为零的情况，并且能够加速某些操作，这让解决某个问题所需要的神经元数量也有所减少。这个bias也是一个数字，有些时候是一个常量（经常是-1或者1），有些时候会有所变化。这个总和最终被输入到一个激活函数，这个激活函数的输出最终就成为这个神经元的输出。

\subsubsection{卷积神经元}

和前馈神经元非常相似，除了它们只跟前一神经细胞层的部分神经元有连接。因为它们不是和某些神经元随机连接的，而是与特定范围内的神经元相连接，通常用来保存空间信息。这让它们对于那些拥有大量局部信息，比如图像数据、语音数据（但多数情况下是图像数据），会非常实用。

\subsubsection{解卷积神经元}

恰好相反：它们是通过跟下一神经细胞层的连接来解码空间信息。这两种神经元都有很多副本，它们都是独立训练的；每个副本都有自己的权重，但连接方式却完全相同。可以认为，这些副本是被放在了具备相同结构的不同的神经网络中。这两种神经元本质上都是一般意义上的神经元，但是，它们的使用方式却不同。

\subsubsection{池化神经元和插值神经元}

经常和卷积神经元结合起来使用。它们不是真正意义上的神经元，只能进行一些简单的操作。

池化神经元接受到来自其它神经元的输出过后，决定哪些值可以通过，哪些值不能通过。在图像领域，可以理解成是把一个图像缩小了（在查看图片的时候，一般软件都有一个放大、缩小的功能；这里的图像缩小，就相当于软件上的缩小图像；也就是说我们能看到图像的内容更加少了；在这个池化的过程当中，图像的大小也会相应地减少）。这样，你就再也不能看到所有的像素了，池化函数会知道什么像素该保留，什么像素该舍弃。

插值神经元恰好是相反的操作：它们获取一些信息，然后映射出更多的信息。额外的信息都是按照某种方式制造出来的，这就好像在一张小分辨率的图片上面进行放大。插值神经元不仅仅是池化神经元的反向操作，而且，它们也是很常见，因为它们运行非常快，同时，实现起来也很简单。池化神经元和插值神经元之间的关系，就像卷积神经元和解卷积神经元之间的关系。

\subsubsection{均值神经元和标准方差神经元（Mean and standard deviation cells）(作为概率神经元它们总是成对的出现)}
是一类用来描述数据概率分布的神经元。均值就是所有值的平均值，而标准方差描述的是这些数据偏离（两个方向）均值有多远。比如：一个用于图像处理的概率神经元可以包含一些信息，比如：在某个特定的像素里面有多少红色。举个例来说，均值可能是0.5，同时标准方差是0.2。当要从这些概率神经元取样的时候，你可以把这些值输入到一个高斯随机数生成器，这样就会生成一些分布在0.4和0.6之间的值；值离0.5越远，对应生成的概率也就越小。它们一般和前一神经元层或者下一神经元层是全连接，而且，它们没有偏差（bias）。

\subsubsection{循环神经元}

不仅仅在神经细胞层之间有连接，而且在时间轴上也有相应的连接。每一个神经元内部都会保存它先前的值。它们跟一般的神经元一样更新，但是，具有额外的权重：与当前神经元之前值之间的权重，还有大多数情况下，与同一神经细胞层各个神经元之间的权重。当前值和存储的先前值之间权重的工作机制，与非永久性存储器（比如RAM）的工作机制很相似，继承了两个性质：

    第一，维持一个特定的状态；
    第二：如果不对其持续进行更新（输入），这个状态就会消失。

    由于先前的值是通过激活函数得到的，而在每一次的更新时，都会把这个值和其它权重一起输入到激活函数，因此，信息会不断地流失。实际上，信息的保存率非常的低，以至于仅仅四次或者五次迭代更新过后，几乎之前所有的信息都会流失掉。
