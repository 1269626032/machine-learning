%% \documentclass{ctexart}

%% \usepackage{amsmath}
%% \usepackage{amsthm}
%% \usepackage{amsfonts}

%% \title{Some Mathematical Aspects of Machine Learling}
%% \author{王逸舟}
%% \date{2018年12月31日}

%% \begin{document}

%% \maketitle
%% \tableofcontents

%% \section{Optimizing Neural Networks in the Equivalence Class Space}

%% \subsection{概念}
%% \subsubsection{机器学习}
\section{机器学习}
Langley（1996) 定义的机器学习是“机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能”。（Machine learning is a science of the artificial. The field's main objects of study are artifacts, specifically algorithms that improve their performance with experience.'）

Tom Mitchell的机器学习(1997)对信息论中的一些概念有详细的解释,其中定义机器学习时提到，“机器学习是对能通过经验自动改进的计算机算法的研究”。（Machine Learning is the study of computer algorithms that improve automatically through experience.）

Alpaydin（2004）同时提出自己对机器学习的定义，“机器学习是用数据或以往的经验，以此优化计算机程序的性能标准。”（Machine learning is programming computers to optimize a performance criterion using example data or past experience.）
\subsection{十大经典机器学习算法}

\subsubsection{决策树}
根据一些 feature（特征） 进行分类，每个节点提一个问题，通过判断，将数据分为两类，再继续提问。这些问题是根据已有数据学习出来的，再投入新数据的时候，就可以根据这棵树上的问题，将数据划分到合适的叶子上。
\subsubsection{随机森林}
在源数据中随机选取数据，组成几个子集：
S矩阵是源数据，有1-N条数据，A、B、C 是feature，最后一列C是类别：
由S随机生成M个子矩阵：
这M个子集得到 M 个决策树：将新数据投入到这M个树中，得到M个分类结果，计数看预测成哪一类的数目最多，就将此类别作为最后的预测结果。
\subsubsection{逻辑回归}
当预测目标是概率这样的，值域需要满足大于等于0，小于等于1的，这个时候单纯的线性模型是做不到的，因为在定义域不在某个范围之内时，值域也超出了规定区间。
那么怎么得到这样的模型呢？

这个模型需要满足两个条件 “大于等于0”，“小于等于1” 。大于等于0 的模型可以选择绝对值，平方值，这里用指数函数，一定大于0；小于等于1 用除法，分子是自己，分母是自身加上1，那一定是小于1的了。

(1)\(p \geq 0\)

\[p=exp(\beta_0+\beta_1 age)\]

(2)\(p\leq 1\)

\[p=\frac{exp(\beta_0+\beta_1age)}{exp(\beta_0+\beta_1age)+1}\]
于是就得到

\[\ln \left(\frac{p}{1-p}\right)=\beta_0+\beta_1age\]
\subsubsection{支持向量机}
要将两类分开，想要得到一个超平面，最优的超平面是到两类的 margin 达到最大，margin就是超平面与离它最近一点的距离

将这个超平面表示成一个线性方程，在线上方的一类，都大于等于1，另一类小于等于－1

\(g(x)\geq 1,\forall x \in class 1\)\\
\(g(x)\leq 1,\forall x \in class2\)
\subsubsection{朴素贝叶斯}
\subsubsection{K近邻算法}
给一个新的数据时，离它最近的 k 个点中，哪个类别多，这个数据就属于哪一类。
\subsubsection{K均值算法}
先要将一组数据，分为三类，粉色数值大，黄色数值小 。最开始先初始化，这里面选了最简单的 3，2，1 作为各类的初始值 。剩下的数据里，每个都与三个初始值计算距离，然后归类到离它最近的初始值所在类别。

分好类后，计算每一类的平均值，作为新一轮的中心点

几轮之后，分组不再变化了，就可以停止了：
\subsubsection{Adaboost}
Adaboost 是 Boosting 的方法之一。Boosting就是把若干个分类效果并不好的分类器综合起来考虑，会得到一个效果比较好的分类器。

下图，左右两个决策树，单个看是效果不怎么好的，但是把同样的数据投入进去，把两个结果加起来考虑，就会增加可信度。
\subsubsection{神经网络}
Neural Networks适合一个input可能落入至少两个类别里：NN由若干层神经元，和它们之间的联系组成。 第一层是input层，最后一层是output层。在hidden层和output层都有自己的classifier。

input输入到网络中，被激活，计算的分数被传递到下一层，激活后面的神经层，最后output层的节点上的分数代表属于各类的分数，下图例子得到分类结果为class 1；同样的input被传输到不同的节点上，之所以会得到不同的结果是因为各自节点有不同的weights 和bias，这也就是forward propagation。
\subsubsection{马尔可夫}

Markov Chains由state（状态）和transitions（转移）组成。例子，根据这一句话 ‘the quick brown fox jumps over the lazy dog’，要得到markov chains。

步骤，先给每一个单词设定成一个状态，然后计算状态间转换的概率。

这是一句话计算出来的概率，当你用大量文本去做统计的时候，会得到更大的状态转移矩阵，例如the后面可以连接的单词，及相应的概率。

